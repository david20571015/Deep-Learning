{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare_train.txt', 'r', encoding='UTF-8') as f:\n",
    "    train_text = f.read()\n",
    "\n",
    "with open('shakespeare_valid.txt', 'r', encoding='UTF-8') as f:\n",
    "    valid_text = f.read()\n",
    "\n",
    "vocab_to_int = tf.keras.layers.StringLookup(\n",
    "    output_mode='int',\n",
    "    vocabulary=list(set(train_text)),\n",
    ")\n",
    "\n",
    "int_to_vocab = tf.keras.layers.StringLookup(\n",
    "    output_mode='int',\n",
    "    vocabulary=vocab_to_int.get_vocabulary(),\n",
    "    invert=True,\n",
    ")\n",
    "\n",
    "train_data = tf.strings.unicode_split(train_text, 'UTF-8')\n",
    "train_data = vocab_to_int(train_data)\n",
    "\n",
    "valid_data = tf.strings.unicode_split(valid_text, 'UTF-8')\n",
    "valid_data = vocab_to_int(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 20\n",
    "BATCI_SIZE = 1024\n",
    "\n",
    "\n",
    "def create_ds(train_data, valid_data, seq_length, batch_size):\n",
    "    # input seq[i:i+SEQ_LENGTH] -> output seq[i+SEQ_LENGTH]\n",
    "    train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        train_data[:-1],\n",
    "        train_data[seq_length:],\n",
    "        seq_length,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    ).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        valid_data[:-1],\n",
    "        valid_data[seq_length:],\n",
    "        seq_length,\n",
    "        batch_size=batch_size,\n",
    "    ).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, valid_ds\n",
    "\n",
    "\n",
    "train_ds, valid_ds = create_ds(train_data, valid_data, SEQ_LENGTH, BATCI_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "RNN_TYPE = 'lstm'\n",
    "\n",
    "if RNN_TYPE == 'rnn':\n",
    "    rnn_layer = tf.keras.layers.SimpleRNN(HIDDEN_SIZE)\n",
    "else:  # RNN_TYPE == 'lstm'\n",
    "    rnn_layer = tf.keras.layers.LSTM(HIDDEN_SIZE)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_to_int.vocabulary_size(), 32),\n",
    "    rnn_layer,\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(int_to_vocab.vocabulary_size()),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, y, loss_fn, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y, y_pred)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, x, y, loss_fn):\n",
    "    y_pred = model(x, training=False)\n",
    "    loss = loss_fn(y, y_pred)\n",
    "\n",
    "    test_loss(loss)\n",
    "    test_accuracy(y, y_pred)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def predict(\n",
    "    input_seq: tf.Tensor,\n",
    "    size: int = 1,\n",
    "    model: tf.keras.Model = model,\n",
    "):\n",
    "    for _ in range(size):\n",
    "        pred = model(input_seq)\n",
    "        pred = tf.random.categorical(pred, num_samples=1)\n",
    "        input_seq = tf.concat([input_seq, pred], axis=-1)\n",
    "    return input_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = next(iter(train_ds))[0][:4]\n",
    "sample_text = tf.strings.reduce_join(int_to_vocab(sample_data), axis=-1)\n",
    "print('Sample text:', sample_text.numpy())\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "log_dir = os.path.join('logs', f'{RNN_TYPE}_{HIDDEN_SIZE}_seq_{SEQ_LENGTH}')\n",
    "train_log_dir = os.path.join(log_dir, 'train')\n",
    "test_log_dir = os.path.join(log_dir, 'test')\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "\n",
    "with train_summary_writer.as_default():\n",
    "    tf.summary.text('sample_text', sample_text, step=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    with tqdm(\n",
    "            train_ds,\n",
    "            total=train_ds.cardinality().numpy(),\n",
    "            desc=f'Epoch {epoch} / {EPOCHS} @ train',\n",
    "            dynamic_ncols=True,\n",
    "    ) as pbar:\n",
    "        for x, y in pbar:\n",
    "            train_step(model, x, y, loss_fn, optimizer)\n",
    "\n",
    "            pbar.set_postfix_str(\n",
    "                f'loss: {train_loss.result():.6f}, accuracy: {train_accuracy.result():.6f}'\n",
    "            )\n",
    "\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "        if epoch % (EPOCHS // 5) == 0:\n",
    "            pred = predict(sample_data, 100)\n",
    "            pred = int_to_vocab(pred)\n",
    "            pred = tf.strings.reduce_join(pred, axis=-1)\n",
    "            tf.summary.text('breakpoint_text', pred, step=epoch)\n",
    "\n",
    "    with tqdm(\n",
    "            valid_ds,\n",
    "            total=valid_ds.cardinality().numpy(),\n",
    "            desc=f'Epoch {epoch} / {EPOCHS} @ test',\n",
    "            dynamic_ncols=True,\n",
    "    ) as pbar:\n",
    "        for x, y in pbar:\n",
    "            test_step(model, x, y, loss_fn)\n",
    "\n",
    "            pbar.set_postfix_str(\n",
    "                f'loss: {test_loss.result():.6f}, accuracy: {test_accuracy.result():.6f}'\n",
    "            )\n",
    "\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prime_text(text, prime_size):\n",
    "    prime_seq = tf.strings.unicode_split(text, 'UTF-8')\n",
    "    prime_seq = vocab_to_int(prime_seq)\n",
    "    prime_seq = tf.expand_dims(prime_seq, axis=0)\n",
    "\n",
    "    pred = predict(prime_seq, prime_size)\n",
    "\n",
    "    pred = int_to_vocab(pred)\n",
    "    pred = tf.strings.reduce_join(pred)\n",
    "    return pred.numpy().decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prime_result = prime_text('JULIET', 500)\n",
    "\n",
    "print(prime_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('dlhw3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17f8d68d433a1dfb4fe5f8b50c9b6033a7d74fef4373dff4a2c9faa0853c7c3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
